{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_DIR = os.path.abspath('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the path to the directory this file is in\n",
    "env_path = os.path.join(PROJ_DIR, '.env')\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and import the learner_datasets module\n",
    "try:\n",
    "    from learner_datasets import get_dataset_info\n",
    "    from evaluate import format_sentence_info\n",
    "except ImportError:\n",
    "    # if the module is not found, add the src directory to the path\n",
    "    import sys\n",
    "    sys.path.append(os.path.join(PROJ_DIR, 'src'))\n",
    "\n",
    "    from learner_datasets import get_dataset_info\n",
    "    from evaluate.evaluate import format_sentence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read best few_shot results\n",
    "df = pd.read_csv('../best_dev_set_results.csv')\n",
    "# select only rows where split == 'wibea-dev'\n",
    "df = df[df['split'] == 'wibea-dev']\n",
    "\n",
    "open_source_model_template = 'wibea_dataset_dev_{model}_{prompt_type}_{prompt_index}_beams=1_temp=0.1_topk=50_topp=1.0'\n",
    "gpt_model_template = 'wibea_dataset_dev_{model}_{prompt_type}_{prompt_index}_temp=0.1_topp=1.0'\n",
    "cohere_model_template = 'wibea_dataset_dev_{model}_{prompt_type}_{prompt_index}_temp=0.1_topk=50_topp=1.0'\n",
    "\n",
    "few_shot_base_path = os.path.join(PROJ_DIR, 'paper_output', 'output_few_shot_dev')\n",
    "zero_shot_base_path = os.path.join(PROJ_DIR, 'paper_output', 'output_zero_shot_dev')\n",
    "\n",
    "model_paths = []\n",
    "for index, row in df.iterrows():\n",
    "    model = row['model'].split('/')[-1]\n",
    "\n",
    "    template = open_source_model_template\n",
    "    if 'gpt' in model:\n",
    "        template = gpt_model_template\n",
    "    elif 'command' in model:\n",
    "        template = cohere_model_template\n",
    "\n",
    "    prompt_type_index = row['prompt_type_index']\n",
    "    prompt_type, prompt_index = prompt_type_index.split('_')\n",
    "\n",
    "    prompt_type = prompt_type.replace('-', '_')\n",
    "\n",
    "    # need to reduce prompt_index by 1 when mapping to file paths\n",
    "    prompt_index = int(prompt_index) - 1\n",
    "\n",
    "    base_path = few_shot_base_path\n",
    "    if prompt_type == '0_shot':\n",
    "        prompt_type = 'zero_shot'\n",
    "        base_path = zero_shot_base_path\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        base_path,\n",
    "        template.format(model=model, prompt_type=prompt_type, prompt_index=prompt_index),\n",
    "        \"run_1\",)\n",
    "    model_paths.append(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model_paths:\n",
    "    print(m)\n",
    "    assert os.path.exists(m)\n",
    "\n",
    "    for cefr in ['A', 'B', 'C', 'N']:\n",
    "        assert os.path.exists(os.path.join(m, f'hyp_post_{cefr}.m2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyp_cefr_level(sub_dir, cefr_level):\n",
    "    base_folder = os.path.dirname(sub_dir)\n",
    "    config_path = os.path.join(base_folder, \"config.yaml\")\n",
    "\n",
    "    model_hyp_file = os.path.join(sub_dir, f\"hyp_post_{cefr_level}.m2\")\n",
    "\n",
    "    # run errant compare\n",
    "    corpora_path = os.environ.get(\"CORPORA\")\n",
    "    ref_file = os.path.join(corpora_path, f'wi+locness/m2/{cefr_level}.dev.auto.m2')\n",
    "    errant_compare_args = [\n",
    "        \"errant_compare\",\n",
    "        \"-hyp\",\n",
    "        model_hyp_file,\n",
    "        \"-ref\",\n",
    "        ref_file,\n",
    "        \"-cat\",\n",
    "        \"3\",\n",
    "        \"-v\"\n",
    "    ]\n",
    "    # subprocess.run(errant_compare_args)\n",
    "    stdoutput = subprocess.check_output(errant_compare_args).decode(\"utf-8\")\n",
    "\n",
    "    evaluation_components = stdoutput.split(\n",
    "            \"=========== Span-Based Correction ============\"\n",
    "        )\n",
    "\n",
    "    sentence_info = evaluation_components[0].split(\n",
    "        \"----------------------------------------\"\n",
    "    )\n",
    "    sentence_info = format_sentence_info(sentence_info[1:])\n",
    "\n",
    "    per_error_results = evaluation_components[1].split(\"\\n\")[1:-2]\n",
    "    # list to pandas dataframe\n",
    "    per_error_results = [x.split() for x in per_error_results]\n",
    "    df = pd.DataFrame(per_error_results)\n",
    "    # make first row the header and drop it\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0)\n",
    "\n",
    "    overall_results = evaluation_components[-1].split(\"\\n\")\n",
    "    header = overall_results[1].split(\"\\t\")\n",
    "    values = overall_results[2].split(\"\\t\")\n",
    "    results = dict(zip(header, values))\n",
    "\n",
    "    # save all info\n",
    "    data = {\n",
    "        \"sentence_results\": sentence_info,\n",
    "        \"per_error_results\": per_error_results,\n",
    "        \"overall_results\": results,\n",
    "    }\n",
    "\n",
    "    # all_results_file = os.path.join(sub_dir, f\"results_hyp_post_{cefr_level}.json\")\n",
    "    # with open(all_results_file, \"wt\") as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cefr_levels = ['A', 'B', 'C', 'N']\n",
    "\n",
    "columns = ['model', 'cefr', 'precision', 'recall', 'f05']\n",
    "all_results = {}\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for sub_dir in model_paths:\n",
    "    print(sub_dir)\n",
    "    model = os.path.basename(os.path.dirname(sub_dir))\n",
    "    model = model.split('_')[3]\n",
    "    for cefr_level in cefr_levels:\n",
    "        try:\n",
    "            results = evaluate_hyp_cefr_level(sub_dir, cefr_level)\n",
    "            all_results[cefr_level] = results\n",
    "            row = [\n",
    "                model,\n",
    "                cefr_level,\n",
    "                results['Prec'],\n",
    "                results['Rec'],\n",
    "                results['F0.5']]\n",
    "            all_rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Failed for {sub_dir} and {cefr_level}\")\n",
    "            break\n",
    "    \n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=columns)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert f05 column to float\n",
    "df['f05'] = df['f05'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_csv(os.path.join('..', 'cefr_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gec-prompting-errant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
